"use strict";(self.webpackChunkmind_mash=self.webpackChunkmind_mash||[]).push([[1803],{73572:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>a});var s=i(74848),t=i(28453);const r={id:"llms-0",title:"The Illusion of AI Thinking"},l=void 0,o={id:"notes/Information Technology/Artificial Intelligence/LLMs/llms-0",title:"The Illusion of AI Thinking",description:"Report on \u201cThe Illusion of AI Thinking\u201d",source:"@site/docs/notes/Information Technology/Artificial Intelligence/LLMs/The Illusion of AI Thinking.md",sourceDirName:"notes/Information Technology/Artificial Intelligence/LLMs",slug:"/notes/Information Technology/Artificial Intelligence/LLMs/llms-0",permalink:"/MindMash/docs/notes/Information Technology/Artificial Intelligence/LLMs/llms-0",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"llms-0",title:"The Illusion of AI Thinking"},sidebar:"notesSidebar",previous:{title:"7-Eleven\u2019s Agentic Marketing Assistant",permalink:"/MindMash/docs/notes/Information Technology/Artificial Intelligence/LLMs/llms-1"},next:{title:"\ud83d\udcd6 What is Machine Learning",permalink:"/MindMash/docs/notes/Information Technology/Artificial Intelligence/\ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Google - 01 - Intro to ML/google-unit-1"}},c={},a=[{value:"Report on \u201cThe Illusion of AI Thinking\u201d",id:"report-on-the-illusion-of-ai-thinking",level:3},{value:"1. Context &amp; Motivation",id:"1-context--motivation",level:2},{value:"2. Experimental Setup",id:"2-experimental-setup",level:2},{value:"3. Key Findings",id:"3-key-findings",level:2},{value:"4. Interpretation",id:"4-interpretation",level:2},{value:"5. Implications for Practitioners",id:"5-implications-for-practitioners",level:2},{value:"6. Future Directions Suggested by Apple",id:"6-future-directions-suggested-by-apple",level:2},{value:"Bottom Line",id:"bottom-line",level:2}];function d(e){const n={blockquote:"blockquote",br:"br",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h3,{id:"report-on-the-illusion-of-ai-thinking",children:"Report on \u201cThe Illusion of AI Thinking\u201d"}),"\n",(0,s.jsx)(n.p,{children:"by Apple ML Research, June 2025"}),"\n",(0,s.jsx)("iframe",{src:"https://www.youtube.com/embed/fGcfJ9J_Faw",title:"Il crollo del ragionamento dell'intelligenza artificiale (di Apple)",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0,className:"video-holidays"}),"\n",(0,s.jsx)(n.h2,{id:"1-context--motivation",children:"1. Context & Motivation"}),"\n",(0,s.jsxs)(n.p,{children:["Recent \u201clarge reasoning models\u201d (LRMs) expose an optional ",(0,s.jsx)(n.em,{children:"thinking/synching"})," mode in which the model emits internal chain-of-thought tokens. Public leaderboard results (ARC-AGI, MATH500, etc.) show large jumps in accuracy when this mode is enabled, so many practitioners assume LRMs genuinely ",(0,s.jsx)(n.em,{children:"reason"})," in a human-like way. Apple\u2019s paper questions that assumption and asks:"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["Are LRMs truly reasoning, or merely creating a convincing ",(0,s.jsx)(n.em,{children:"illusion"})," of reasoning?"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2-experimental-setup",children:"2. Experimental Setup"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Aspect"}),(0,s.jsx)(n.th,{children:"Details"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Models"})}),(0,s.jsx)(n.td,{children:"Claude 3.7 (non-sync vs. sync), Claude Sonnet 4, DeepSeek-R1, Gemini 2.5 preview, etc."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Tasks"})}),(0,s.jsx)(n.td,{children:"Four classical algorithmic puzzles chosen for precise complexity control and verifiable solutions: 1) Tower of Hanoi, 2) Checker jumping, 3) River-crossing, 4) Block-world rearrangement."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Complexity Axis"})}),(0,s.jsxs)(n.td,{children:["Number of objects/disks to move (1 \u2192 12). Defines three regimes: ",(0,s.jsx)(n.strong,{children:"Low"}),", ",(0,s.jsx)(n.strong,{children:"Medium"}),", ",(0,s.jsx)(n.strong,{children:"High"})," complexity."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Metrics"})}),(0,s.jsx)(n.td,{children:"Accuracy, number of reasoning tokens, wall-clock latency."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Interventions"})}),(0,s.jsx)(n.td,{children:"1. Enable/disable thinking mode; 2. Vary thinking-token budget (8 k \u2192 32 k); 3. Provide step-by-step solution templates (few-shot / RL fine-tuning)."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"3-key-findings",children:"3. Key Findings"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Three-phase performance curve"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:"Low complexity"})," (\u2264 2 objects)",(0,s.jsx)(n.br,{}),"\n","non-thinking baseline already solves tasks; thinking adds no value and is sometimes slightly worse."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:"Medium complexity"})," (\u2248 4 \u2013 8 objects)",(0,s.jsx)(n.br,{}),"\n","thinking mode strongly outperforms; more tokens \u2192 higher success."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.em,{children:"High complexity"})," (\u2265 10 objects)",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"both modes collapse to ~0 accuracy"}),(0,s.jsx)(n.br,{}),"\n","Surprisingly, the model emits ",(0,s.jsx)(n.em,{children:"fewer"})," thinking tokens beyond this point; evidence it \u201cgives up\u201d instead of trying harder."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Token-count inversion"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Expectations: harder task \u2192 longer chain-of-thought."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Observation: after a sharp threshold, token length ",(0,s.jsx)(n.em,{children:"drops"})," even as complexity rises.",(0,s.jsx)(n.br,{}),"\n","This contradicts the view that LRMs search deeper when challenged."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ineffectiveness of additional guidance"}),"\nProviding explicit solution templates or RL fine-tuning did ",(0,s.jsx)(n.strong,{children:"not"})," move the collapse point. Models still failed at the same complexity tier."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Memorisation vs. reasoning"}),"\nClaude 3.7 could execute ~100 flawless moves on the ",(0,s.jsx)(n.em,{children:"well-studied"})," Tower of Hanoi but failed after < 5 moves on an obscure river-crossing variant requiring only 11 total moves, indicating recall of memorised patterns, not general reasoning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tool / code execution sidesteps the issue"}),"\nWhen the LRM delegates planning to external code (Python/C++ solver) it instantly produces perfect solutions, highlighting that the failure lies in transformer-based reasoning rather than the tasks themselves."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"4-interpretation",children:"4. Interpretation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Illusion Thesis"}),(0,s.jsx)(n.br,{}),"\n","The apparent \u201creasoning\u201d bump at medium complexity is fragile and narrow. Beyond a modest threshold, LRMs abandon systematic search and revert to shorter, often hallucinatory outputs."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pattern-matching Dominance"}),(0,s.jsx)(n.br,{}),"\n","Success on famous benchmarks stems largely from memorised algorithmic templates seen in pre-training data; unfamiliar but isomorphic tasks expose the weakness."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scaling Limits"}),(0,s.jsx)(n.br,{}),"\n","Merely increasing context window (8 k \u2192 32 k) or parameter count does ",(0,s.jsx)(n.strong,{children:"not"})," extend the reasoning frontier."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement Learning Limits"}),(0,s.jsx)(n.br,{}),"\n","RL fine-tuning improves style adherence and detox, but doesn\u2019t endow models with fundamentally deeper deductive ability."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Practical Implication"}),(0,s.jsx)(n.br,{}),"\n","For workloads \u2248 medium complexity (e.g. 2-to-3-step logical problems) thinking mode is worthwhile; for truly intricate algorithmic planning, direct code execution or specialised symbolic solvers remain essential."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"5-implications-for-practitioners",children:"5. Implications for Practitioners"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Benchmark responsibly"}),(0,s.jsx)(n.br,{}),"\n","Relying on saturated public leaderboards can mislead; craft domain-specific, unpublished tests to measure genuine reasoning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Budget thinking tokens prudently"}),(0,s.jsx)(n.br,{}),"\n","Enable thinking when tasks sit in the \u201cmedium\u201d band; disable it for trivial queries to save latency & cost."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Augment with tools"}),(0,s.jsx)(n.br,{}),"\n","For complex combinatorial planning, integrate external solvers rather than expecting pure-LLM deduction."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Monitor token-usage curves"}),(0,s.jsx)(n.br,{}),"\n","A sudden drop in reasoning-token length is a red-flag that the model has silently abandoned the problem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"6-future-directions-suggested-by-apple",children:"6. Future Directions Suggested by Apple"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architectural research"})," into mechanisms that ",(0,s.jsx)(n.em,{children:"maintain"})," or ",(0,s.jsx)(n.em,{children:"adaptively expand"})," reasoning depth instead of collapsing."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Curriculum and data"})," targeting under-represented algorithmic patterns."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transparency"}),": expose chain-of-thought for proprietary models to allow finer-grained diagnosis."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"bottom-line",children:"Bottom Line"}),"\n",(0,s.jsx)(n.p,{children:"Apple\u2019s \u201cIllusion of AI Thinking\u201d delivers a sobering reality-check: today\u2019s LRMs exhibit genuine-looking reasoning only in a narrow Goldilocks zone of problem complexity. Beyond that, they default to short, often wrong outputs rather than \u201ctrying harder.\u201d Effective deployment therefore requires calibrated use of thinking budgets, bespoke benchmarks, and\u2014crucially\u2014hybrid systems that delegate heavy logical lifting to explicit code or symbolic engines."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);