<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">🧪 LayoutLMv3 | MindMash</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://github.com/MindMash/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://github.com/MindMash/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="🧪 LayoutLMv3 | MindMash"><meta data-rh="true" name="description" content="Last Updated in Date: 10/01/2025"><meta data-rh="true" property="og:description" content="Last Updated in Date: 10/01/2025"><link data-rh="true" rel="icon" href="/MindMash/img/planet-earth.png"><link data-rh="true" rel="canonical" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1"><link data-rh="true" rel="alternate" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1" hreflang="en"><link data-rh="true" rel="alternate" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/MindMash/assets/css/styles.840d84ac.css">
<script src="/MindMash/assets/js/runtime~main.dce65f8d.js" defer="defer"></script>
<script src="/MindMash/assets/js/main.1e46c6d1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/MindMash/"><div class="navbar__logo"><img src="/MindMash/img/logo.svg" alt="MindMash Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/MindMash/img/logo.svg" alt="MindMash Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">MindMash</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/MindMash/docs/notes/intro">Notes</a><a class="navbar__item navbar__link" href="/MindMash/docs/tracking/intro">Tracking</a><a class="navbar__item navbar__link" href="/MindMash/docs/private/intro-private">Private</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/DanielePassabi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/MindMash/docs/notes/intro">Introduction to Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Economics and Finance/economia-e-finanza-01">Economics and Finance</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Gym/Consigli Trainer/gym-trainer-insights-03">Gym</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">Information Technology</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">Artificial Intelligence</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">🔡 Glossary</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/LLMs/llms-1">LLMs</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-1">👨🏻‍🏫 Google - 01 - Intro to ML</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-1">📖 What is Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-2">📖 Supervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-additional-3">📖 Self-Supervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-model-1">🧪 LayoutLMv3</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-1">👨🏻‍🏫 Google - 02 - ML</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 HuggingFace - Agents/hf-unit-1">👨🏻‍🏫 HuggingFace - Agents</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/🗃️ Unsorted Notes/ai-01">🗃️ Unsorted Notes</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Python/python-01">Python</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Web Development/wd-intro">Web Development</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Italian Literature/il-intro">Italian Literature</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/MindMash/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Information Technology</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Artificial Intelligence</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">👨🏻‍🏫 Google - 01 - Intro to ML</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">🧪 LayoutLMv3</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>🧪 LayoutLMv3</h1></header><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Warning: Last Update</div><div class="admonitionContent_BuS1"><p>Last Updated in Date: 10/01/2025</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="️-useful-materials">➡️ <strong>Useful Materials</strong><a href="#️-useful-materials" class="hash-link" aria-label="Direct link to ️-useful-materials" title="Direct link to ️-useful-materials">​</a></h2>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/model_doc/layoutlmv3" target="_blank" rel="noopener noreferrer">Huggingface Model</a></li>
<li><a href="https://arxiv.org/pdf/2204.08387" target="_blank" rel="noopener noreferrer">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1️⃣-introduction-and-background">1️⃣ <strong>Introduction and Background</strong><a href="#1️⃣-introduction-and-background" class="hash-link" aria-label="Direct link to 1️⃣-introduction-and-background" title="Direct link to 1️⃣-introduction-and-background">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="document-intelligence-and-the-need-for-layoutlm">Document Intelligence and the Need for LayoutLM<a href="#document-intelligence-and-the-need-for-layoutlm" class="hash-link" aria-label="Direct link to Document Intelligence and the Need for LayoutLM" title="Direct link to Document Intelligence and the Need for LayoutLM">​</a></h3>
<p>In many business and real-world scenarios (e.g., invoices, forms, contracts, academic papers), <strong>documents</strong> contain information in multiple modalities:</p>
<ul>
<li><strong>Textual (language)</strong><br>
<!-- -->The sequence of words, paragraphs, etc.</li>
<li><strong>Visual</strong><br>
<!-- -->The layout of text, tables, images, and other graphical elements on a page.</li>
</ul>
<p>Traditional natural language processing (NLP) solutions focus primarily on text, ignoring critical cues from the <strong>layout</strong> and <strong>visual</strong> elements of documents. However, these elements often convey context and relationships that are essential for tasks like form understanding, information extraction, and document classification.</p>
<p>To address these issues, Microsoft Research introduced a series of models under the <strong>LayoutLM</strong> family:</p>
<ol>
<li><strong>LayoutLM (v1)</strong><br>
<!-- -->Incorporated text and 2D positional embeddings to capture layout information.</li>
<li><strong>LayoutLMv2</strong><br>
<!-- -->Introduced better integration of visual features via a two-stream architecture - one for text and layout, another for image features.</li>
<li><strong>LayoutLMv3</strong><br>
<!-- -->The most recent iteration (as of genuary 2025), refining multimodal fusion by <strong>jointly</strong> learning text, layout, and image representations in a unified transformer architecture.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2️⃣-what-is-layoutlmv3">2️⃣ <strong>What Is LayoutLMv3?</strong><a href="#2️⃣-what-is-layoutlmv3" class="hash-link" aria-label="Direct link to 2️⃣-what-is-layoutlmv3" title="Direct link to 2️⃣-what-is-layoutlmv3">​</a></h2>
<p><strong>LayoutLMv3</strong> is a <strong>pre-trained multimodal Transformer model</strong> designed to handle scanned documents, PDFs, and images that contain text. Its main improvement over previous versions lies in a more unified approach to combining textual and visual features. Rather than treating text and image streams separately, LayoutLMv3 <strong>integrates</strong> them more closely, allowing the model to capture richer cross-modal interactions.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-objectives">Key Objectives<a href="#key-objectives" class="hash-link" aria-label="Direct link to Key Objectives" title="Direct link to Key Objectives">​</a></h3>
<ol>
<li>
<p><strong>Better Text-Layout Integration</strong><br>
<!-- -->Improving upon LayoutLMv2’s two-stream approach by learning a single, unified embedding space for textual and visual information.</p>
</li>
<li>
<p><strong>Enhanced Visual Backbone</strong><br>
<!-- -->Incorporating a stronger vision transformer or CNN backbone to extract high-quality image features, making the model more accurate in visual-heavy tasks (like form layout analysis).</p>
</li>
<li>
<p><strong>Pre-training Efficiency</strong><br>
<!-- -->Optimizing the pre-training strategy to handle large amounts of unlabeled data, thus making fine-tuning more effective across various downstream tasks.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3️⃣-architecture">3️⃣ <strong>Architecture</strong><a href="#3️⃣-architecture" class="hash-link" aria-label="Direct link to 3️⃣-architecture" title="Direct link to 3️⃣-architecture">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="unified-transformer-encoder">Unified Transformer Encoder<a href="#unified-transformer-encoder" class="hash-link" aria-label="Direct link to Unified Transformer Encoder" title="Direct link to Unified Transformer Encoder">​</a></h3>
<p>Unlike LayoutLMv2 (which had a separate image encoder that was fused with text embeddings at specific transformer layers), LayoutLMv3 aims for a <strong>unified transformer encoder</strong>. This means:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="-input-embeddings">➽ Input Embeddings<a href="#-input-embeddings" class="hash-link" aria-label="Direct link to ➽ Input Embeddings" title="Direct link to ➽ Input Embeddings">​</a></h4>
<ul>
<li>
<p><strong>Text Tokens</strong><br>
<!-- -->Standard token embeddings (e.g., from WordPiece or BPE tokenization).</p>
</li>
<li>
<p><strong>Spatial Layout Embeddings</strong><br>
<!-- -->2D coordinates (x, y positions on a page) are discretized and embedded to preserve where text appears.</p>
</li>
<li>
<p><strong>Visual Embeddings</strong><br>
<!-- -->Raw pixels or features from a CNN/Vision Transformer are processed and aligned with textual tokens.</p>
</li>
<li>
<p><strong>Attention Mechanism</strong><br>
<!-- -->A multi-headed self-attention mechanism sees both textual tokens and their corresponding visual embeddings, enabling cross-attention between text and image at every layer.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-fusion-strategies">Multimodal Fusion Strategies<a href="#multimodal-fusion-strategies" class="hash-link" aria-label="Direct link to Multimodal Fusion Strategies" title="Direct link to Multimodal Fusion Strategies">​</a></h3>
<p>One of the defining aspects of LayoutLMv3 is <strong>how</strong> it fuses the different modalities:</p>
<ol>
<li>
<p><strong>Patch Embeddings</strong> (Vision)<br>
<!-- -->Similar to the patch embedding strategy used in ViT (Vision Transformer), the page image is split into patches (like 16×16 pixels), and each patch is embedded into a vector.</p>
</li>
<li>
<p><strong>Token Embeddings</strong> (Language)<br>
<!-- -->The words (or subwords) are tokenized and embedded in the same dimensional space.</p>
</li>
<li>
<p><strong>Positional Encoding</strong><br>
<!-- -->LayoutLMv3 uses both standard transformer positional encodings for sequential text and 2D positional embeddings for layout coordinates.</p>
</li>
<li>
<p><strong>Cross-Modal Attention</strong><br>
<!-- -->Every transformer layer can attend to both textual tokens and visual patches, promoting a holistic representation that captures text, layout, and visual context.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-objectives">Training Objectives<a href="#training-objectives" class="hash-link" aria-label="Direct link to Training Objectives" title="Direct link to Training Objectives">​</a></h3>
<p>LayoutLMv3 typically employs <strong>self-supervised</strong> objectives during pre-training:</p>
<ol>
<li>
<p><strong>Masked Language Modeling (MLM)</strong><br>
<!-- -->Randomly masks out some portion of text tokens and trains the model to predict the masked tokens, leveraging context from both text and vision features.</p>
</li>
<li>
<p><strong>Masked Vision Modeling (MVM)</strong> or <strong>Masked Patch Prediction</strong><br>
<!-- -->Randomly masks out some image patches and trains the model to predict visual features, enabling deeper visual understanding.</p>
</li>
<li>
<p><strong>Text-Image Alignment</strong> (optional in some configurations)<br>
<!-- -->The model may also learn to align textual tokens with the corresponding visual patches representing the same region, reinforcing synergy between text and layout.</p>
</li>
</ol>
<p>These combined objectives encourage the model to capture correlations between text, layout information, and images.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4️⃣-pre-training-dataset-and-process">4️⃣ <strong>Pre-training Dataset and Process</strong><a href="#4️⃣-pre-training-dataset-and-process" class="hash-link" aria-label="Direct link to 4️⃣-pre-training-dataset-and-process" title="Direct link to 4️⃣-pre-training-dataset-and-process">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-sources">Data Sources<a href="#data-sources" class="hash-link" aria-label="Direct link to Data Sources" title="Direct link to Data Sources">​</a></h3>
<p>LayoutLMv3 is typically pre-trained on large-scale document datasets, such as:</p>
<ul>
<li><strong>Public Document Repositories</strong> (e.g., IIT-CDIP, RVL-CDIP).</li>
<li><strong>Web-scraped PDFs</strong> or scanned document corpora.</li>
<li><strong>Internal Enterprise Document Collections</strong> (if available).</li>
</ul>
<p>The <strong>key</strong> is to expose the model to a diverse set of document layouts, fonts, images, tables, and textual content to ensure robust learning of multimodal features.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="large-scale-training">Large-Scale Training<a href="#large-scale-training" class="hash-link" aria-label="Direct link to Large-Scale Training" title="Direct link to Large-Scale Training">​</a></h3>
<ul>
<li><strong>Hardware Requirements</strong><br>
<!-- -->Given the scale and multimodal nature, large GPU/TPU clusters are typically utilized.</li>
<li><strong>Hyperparameters</strong><br>
<!-- -->Often large batch sizes and extended training steps (e.g., tens or hundreds of thousands of steps) are used to ensure the model converges on diverse data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5️⃣-downstream-tasks-and-applications">5️⃣ <strong>Downstream Tasks and Applications</strong><a href="#5️⃣-downstream-tasks-and-applications" class="hash-link" aria-label="Direct link to 5️⃣-downstream-tasks-and-applications" title="Direct link to 5️⃣-downstream-tasks-and-applications">​</a></h2>
<p>LayoutLMv3 can be fine-tuned for a variety of tasks:</p>
<ol>
<li>
<p><strong>Document Classification</strong><br>
<!-- -->Predict the category of a document (invoice, resume, contract, etc.) based on both text and visual layout cues.</p>
</li>
<li>
<p><strong>Form Understanding / Key-Value Pair Extraction</strong><br>
<!-- -->Extract structured information (e.g., name, address, date) from forms or invoices, leveraging the spatial arrangement to identify relevant text.</p>
</li>
<li>
<p><strong>Optical Character Recognition (OCR) Enhancement</strong><br>
<!-- -->Although LayoutLMv3 typically works with OCR’d text, its visual embedding can help in improving error correction or aligning OCR tokens with visual information.</p>
</li>
<li>
<p><strong>Table Extraction</strong><br>
<!-- -->Detect and extract tables from scanned documents, using layout signals to separate rows and columns.</p>
</li>
<li>
<p><strong>Document Question Answering</strong> (DocQA)<br>
<!-- -->Answer questions about a document’s content by understanding the interplay between text, layout, and images.</p>
</li>
<li>
<p><strong>Receipt and Invoice Processing</strong><br>
<!-- -->Automate financial and business workflows by extracting structured data from receipts and invoices.</p>
</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Example: Form Understanding</div><div class="admonitionContent_BuS1"><p>Consider a typical tax form containing fields like “Name,” “Address,” “Income,” etc. LayoutLMv3 can:</p><ol>
<li>Understand where each field label is located (visual + text).</li>
<li>Infer which text token is the filled-out value.</li>
<li>Provide structured key-value pairs for each field.</li>
</ol></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6️⃣-performance-and-benchmarks">6️⃣ <strong>Performance and Benchmarks</strong><a href="#6️⃣-performance-and-benchmarks" class="hash-link" aria-label="Direct link to 6️⃣-performance-and-benchmarks" title="Direct link to 6️⃣-performance-and-benchmarks">​</a></h2>
<p>LayoutLMv3 achieves <strong>state-of-the-art (SOTA) performance</strong> on several public benchmarks such as:</p>
<ul>
<li><strong>FUNSD</strong> (Form Understanding in Noisy Scanned Documents)</li>
<li><strong>SROIE</strong> (Scanned Receipts OCR and Information Extraction)</li>
<li><strong>DocVQA</strong> (Document Visual Question Answering)</li>
<li><strong>RVL-CDIP</strong> (Document Classification)</li>
</ul>
<p>Quantitatively, it outperforms prior LayoutLM versions (v1, v2) and other multimodal baselines by significant margins, demonstrating improved accuracy, F1 scores, or mAP (mean Average Precision), depending on the benchmark.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7️⃣-comparison-with-other-layout-models">7️⃣ <strong>Comparison with Other Layout Models</strong><a href="#7️⃣-comparison-with-other-layout-models" class="hash-link" aria-label="Direct link to 7️⃣-comparison-with-other-layout-models" title="Direct link to 7️⃣-comparison-with-other-layout-models">​</a></h2>
<p>There are other models aimed at multimodal document understanding, such as <strong>DocFormer</strong>, <strong>TILT (Text-Image Layout Transformer)</strong>, <strong>TrOCR</strong> (for OCR), etc. LayoutLMv3 distinguishes itself by:</p>
<ol>
<li>
<p><strong>Unified Transformer</strong><br>
<!-- -->No separate streams for text and image, which leads to more direct cross-modal interactions.</p>
</li>
<li>
<p><strong>Rich Pre-training Strategy</strong><br>
<!-- -->Combines masked language modeling with masked vision modeling.</p>
</li>
<li>
<p><strong>Strong Empirical Performance</strong><br>
<!-- -->Consistently high accuracy across multiple tasks with minimal fine-tuning effort.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="8️⃣-challenges-and-limitations">8️⃣ <strong>Challenges and Limitations</strong><a href="#8️⃣-challenges-and-limitations" class="hash-link" aria-label="Direct link to 8️⃣-challenges-and-limitations" title="Direct link to 8️⃣-challenges-and-limitations">​</a></h2>
<p>Despite its impressive capabilities, LayoutLMv3 has certain challenges:</p>
<ol>
<li>
<p><strong>Computational Resources</strong><br>
<!-- -->Training a large multimodal model is expensive, requiring significant GPU/TPU resources.</p>
</li>
<li>
<p><strong>OCR Dependency</strong><br>
<!-- -->Most use cases still rely on external OCR to convert images to text before feeding them into the model. OCR errors can propagate and degrade performance.</p>
</li>
<li>
<p><strong>Data Privacy</strong><br>
<!-- -->Documents often contain sensitive information, so training or fine-tuning on private corporate data may require additional privacy safeguards.</p>
</li>
<li>
<p><strong>Complex Layouts</strong><br>
<!-- -->Highly complex, multi-page, or heavily tabular documents still present difficulties.</p>
</li>
<li>
<p><strong>Language and Script Support</strong><br>
<!-- -->LayoutLMv3’s performance might vary for different scripts (e.g., Latin vs. non-Latin). Extension to right-to-left scripts or vertical scripts may need special considerations.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="9️⃣-conclusion">9️⃣ <strong>Conclusion</strong><a href="#9️⃣-conclusion" class="hash-link" aria-label="Direct link to 9️⃣-conclusion" title="Direct link to 9️⃣-conclusion">​</a></h2>
<p>LayoutLMv3 stands at the forefront of <strong>multimodal document understanding</strong>, leveraging unified transformer architectures and self-supervised pre-training strategies. By jointly modeling text, layout, and visual information, it achieves superior performance on real-world document tasks-transforming how we handle and extract information from scanned documents, PDFs, and forms. While challenges like high resource requirements and reliance on OCR remain, ongoing research and community contributions continue to refine LayoutLMv3, paving the way for even more robust and accessible document AI solutions.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-additional-3"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">📖 Self-Supervised Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-1"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">📖 Linear Regression</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#️-useful-materials" class="table-of-contents__link toc-highlight">➡️ <strong>Useful Materials</strong></a></li><li><a href="#1️⃣-introduction-and-background" class="table-of-contents__link toc-highlight">1️⃣ <strong>Introduction and Background</strong></a><ul><li><a href="#document-intelligence-and-the-need-for-layoutlm" class="table-of-contents__link toc-highlight">Document Intelligence and the Need for LayoutLM</a></li></ul></li><li><a href="#2️⃣-what-is-layoutlmv3" class="table-of-contents__link toc-highlight">2️⃣ <strong>What Is LayoutLMv3?</strong></a><ul><li><a href="#key-objectives" class="table-of-contents__link toc-highlight">Key Objectives</a></li></ul></li><li><a href="#3️⃣-architecture" class="table-of-contents__link toc-highlight">3️⃣ <strong>Architecture</strong></a><ul><li><a href="#unified-transformer-encoder" class="table-of-contents__link toc-highlight">Unified Transformer Encoder</a></li><li><a href="#multimodal-fusion-strategies" class="table-of-contents__link toc-highlight">Multimodal Fusion Strategies</a></li><li><a href="#training-objectives" class="table-of-contents__link toc-highlight">Training Objectives</a></li></ul></li><li><a href="#4️⃣-pre-training-dataset-and-process" class="table-of-contents__link toc-highlight">4️⃣ <strong>Pre-training Dataset and Process</strong></a><ul><li><a href="#data-sources" class="table-of-contents__link toc-highlight">Data Sources</a></li><li><a href="#large-scale-training" class="table-of-contents__link toc-highlight">Large-Scale Training</a></li></ul></li><li><a href="#5️⃣-downstream-tasks-and-applications" class="table-of-contents__link toc-highlight">5️⃣ <strong>Downstream Tasks and Applications</strong></a></li><li><a href="#6️⃣-performance-and-benchmarks" class="table-of-contents__link toc-highlight">6️⃣ <strong>Performance and Benchmarks</strong></a></li><li><a href="#7️⃣-comparison-with-other-layout-models" class="table-of-contents__link toc-highlight">7️⃣ <strong>Comparison with Other Layout Models</strong></a></li><li><a href="#8️⃣-challenges-and-limitations" class="table-of-contents__link toc-highlight">8️⃣ <strong>Challenges and Limitations</strong></a></li><li><a href="#9️⃣-conclusion" class="table-of-contents__link toc-highlight">9️⃣ <strong>Conclusion</strong></a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">My Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://danielepassabi.github.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My Portfolio<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/daniele-passabi/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">(I do not really need a) Copyright © 2025 MindMash, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>