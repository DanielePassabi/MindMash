<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">📖 Gradient Descent | MindMash</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://github.com/MindMash/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://github.com/MindMash/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="📖 Gradient Descent | MindMash"><meta data-rh="true" name="description" content="➡️ Useful Materials"><meta data-rh="true" property="og:description" content="➡️ Useful Materials"><link data-rh="true" rel="icon" href="/MindMash/img/planet-earth.png"><link data-rh="true" rel="canonical" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1"><link data-rh="true" rel="alternate" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1" hreflang="en"><link data-rh="true" rel="alternate" href="https://github.com/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/MindMash/assets/css/styles.840d84ac.css">
<script src="/MindMash/assets/js/runtime~main.dce65f8d.js" defer="defer"></script>
<script src="/MindMash/assets/js/main.1e46c6d1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/MindMash/"><div class="navbar__logo"><img src="/MindMash/img/logo.svg" alt="MindMash Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/MindMash/img/logo.svg" alt="MindMash Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">MindMash</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/MindMash/docs/notes/intro">Notes</a><a class="navbar__item navbar__link" href="/MindMash/docs/tracking/intro">Tracking</a><a class="navbar__item navbar__link" href="/MindMash/docs/private/intro-private">Private</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/DanielePassabi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/MindMash/docs/notes/intro">Introduction to Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Economics and Finance/economia-e-finanza-01">Economics and Finance</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Gym/Consigli Trainer/gym-trainer-insights-03">Gym</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">Information Technology</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">Artificial Intelligence</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/general-0">🔡 Glossary</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/LLMs/llms-1">LLMs</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 01 - Intro to ML/google-unit-1">👨🏻‍🏫 Google - 01 - Intro to ML</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-1">👨🏻‍🏫 Google - 02 - ML</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-1">📖 Linear Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-addtional-1">📖 Gradient Descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-2">📖 Logistic Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-3">📖 Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-4">📖 Numerical Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-5">📖 Categorical Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-6">📖 Datasets, Generalization, and Overfitting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 HuggingFace - Agents/hf-unit-1">👨🏻‍🏫 HuggingFace - Agents</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/🗃️ Unsorted Notes/ai-01">🗃️ Unsorted Notes</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Python/python-01">Python</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/MindMash/docs/notes/Information Technology/Web Development/wd-intro">Web Development</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/MindMash/docs/notes/Italian Literature/il-intro">Italian Literature</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/MindMash/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Information Technology</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Artificial Intelligence</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">👨🏻‍🏫 Google - 02 - ML</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">📖 Gradient Descent</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>📖 Gradient Descent</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="️-useful-materials">➡️ <strong>Useful Materials</strong><a href="#️-useful-materials" class="hash-link" aria-label="Direct link to ️-useful-materials" title="Direct link to ️-useful-materials">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="original-source">Original Source<a href="#original-source" class="hash-link" aria-label="Direct link to Original Source" title="Direct link to Original Source">​</a></h3>
<p>For a more in-depth explanation of Gradient Descent, we recommend the following video by 3Blue1Brown.</p>
<iframe src="https://www.youtube.com/embed/IHZwWFHWa-w" title="Gradient descent, how neural networks learn | DL2" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" class="video-holidays"></iframe>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="resources-and-further-study">Resources and Further Study<a href="#resources-and-further-study" class="hash-link" aria-label="Direct link to Resources and Further Study" title="Direct link to Resources and Further Study">​</a></h3>
<ul>
<li>Michael Nielsen’s online book on neural networks and deep learning (free and publicly available) offers a detailed walkthrough with code.</li>
<li>Other resources, such as Chris Olah’s blog posts, Distill publications, and various courses, illuminate how modern deep neural networks operate internally.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1️⃣-background-and-motivation">1️⃣ <strong>Background and Motivation</strong><a href="#1️⃣-background-and-motivation" class="hash-link" aria-label="Direct link to 1️⃣-background-and-motivation" title="Direct link to 1️⃣-background-and-motivation">​</a></h2>
<p>Below is a comprehensive, step-by-step explanation of gradient descent and its role in training neural networks, drawing on all the key insights from the video transcript. It covers how a neural network is set up for digit recognition, how the cost function is defined, and how gradient descent iteratively improves the network’s parameters. It also touches on issues like local minima, memorization vs. learning, and how modern networks build upon these ideas.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="handwritten-digit-recognition">Handwritten Digit Recognition<a href="#handwritten-digit-recognition" class="hash-link" aria-label="Direct link to Handwritten Digit Recognition" title="Direct link to Handwritten Digit Recognition">​</a></h3>
<p>A classic benchmark for neural networks is recognizing handwritten digits (0-9) from 28×28 grayscale images. Each pixel in this 28×28 grid has a value between 0 and 1, and these 784 pixel values form the input to the network. The basic goal is that, given a new digit image, the network outputs the correct label - i.e., the digit it represents.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="network-architecture-recap">Network Architecture Recap<a href="#network-architecture-recap" class="hash-link" aria-label="Direct link to Network Architecture Recap" title="Direct link to Network Architecture Recap">​</a></h3>
<p>A typical neural network for this task has:</p>
<ul>
<li>An <strong>input layer</strong> with 784 neurons (one per pixel).</li>
<li>One or more <strong>hidden layers</strong>, where each neuron computes a weighted sum of the previous layer’s activations plus a bias, and then applies an activation function such as the sigmoid (a “squishing” function) or ReLU (rectified linear unit).</li>
<li>An <strong>output layer</strong> with 10 neurons - one for each possible digit (0 through 9).<br>
<!-- -->The network assigns a “brightness” or activation level to each of these 10 output neurons, and the digit corresponding to the brightest neuron is considered the predicted class.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="parameters-weights-and-biases">Parameters (Weights and Biases)<a href="#parameters-weights-and-biases" class="hash-link" aria-label="Direct link to Parameters (Weights and Biases)" title="Direct link to Parameters (Weights and Biases)">​</a></h3>
<p>Each neuron’s behavior depends on its weights (one weight per connection to the previous layer) and a single bias. With two hidden layers of 16 neurons each, plus the input and output layers, there can be around 13,000 parameters in total. Adjusting these parameters is the crux of how the network “learns”.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2️⃣-the-learning-goal-minimizing-cost">2️⃣ <strong>The Learning Goal: Minimizing Cost</strong><a href="#2️⃣-the-learning-goal-minimizing-cost" class="hash-link" aria-label="Direct link to 2️⃣-the-learning-goal-minimizing-cost" title="Direct link to 2️⃣-the-learning-goal-minimizing-cost">​</a></h2>
<p>After randomly initializing all weights and biases, the network initially performs poorly. We need a systematic way to make it better using labeled data - images of digits plus the correct label for each digit.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-define-a-cost-function">1. Define a Cost Function<a href="#1-define-a-cost-function" class="hash-link" aria-label="Direct link to 1. Define a Cost Function" title="Direct link to 1. Define a Cost Function">​</a></h3>
<ul>
<li>For each training example, measure the difference between the network’s predicted output and the desired output.</li>
<li>A common choice is to sum up the squares of these differences (sometimes called a squared error).</li>
<li>If the network classifies an example correctly and with high confidence, this sum is small. If it’s wildly off, the sum is large.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-average-over-the-entire-training-set">2. Average Over the Entire Training Set<a href="#2-average-over-the-entire-training-set" class="hash-link" aria-label="Direct link to 2. Average Over the Entire Training Set" title="Direct link to 2. Average Over the Entire Training Set">​</a></h3>
<ul>
<li>Because tens of thousands of labeled images are available (the MNIST dataset), we typically look at the <strong>average</strong> cost across all those samples.</li>
<li>This average cost is a single scalar number that captures how “lousy” the current set of weights and biases is.</li>
<li>Lowering this average cost means the network is performing better across all examples, not just one.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-local-minima-vs-global-minima">3. Local Minima vs. Global Minima<a href="#3-local-minima-vs-global-minima" class="hash-link" aria-label="Direct link to 3. Local Minima vs. Global Minima" title="Direct link to 3. Local Minima vs. Global Minima">​</a></h3>
<ul>
<li>In principle, there could be many ways to adjust the weights such that the network improves. Each “valley” in the cost-function landscape corresponds to a local minimum.</li>
<li>Which local minimum the network ends up in can depend on its initial random parameter settings and the details of the training algorithm.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3️⃣-the-core-algorithm-gradient-descent">3️⃣ <strong>The Core Algorithm: Gradient Descent</strong><a href="#3️⃣-the-core-algorithm-gradient-descent" class="hash-link" aria-label="Direct link to 3️⃣-the-core-algorithm-gradient-descent" title="Direct link to 3️⃣-the-core-algorithm-gradient-descent">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-idea">Key Idea<a href="#key-idea" class="hash-link" aria-label="Direct link to Key Idea" title="Direct link to Key Idea">​</a></h3>
<p>Gradient descent is the iterative process for finding which direction to shift each weight and bias so that the cost function goes down. Even though the cost function might live in a 13,000-dimensional space, the same intuition from single-variable calculus extends to higher dimensions:</p>
<ol>
<li>
<p><strong>Compute the Gradient</strong></p>
<ul>
<li>The <strong>gradient</strong> tells you which direction in parameter-space (i.e., which combination of weight/bias adjustments) most rapidly <strong>increases</strong> the cost.</li>
<li>Its negative (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo></mrow><annotation encoding="application/x-tex">-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">−</span></span></span></span>gradient) indicates which direction decreases the cost the fastest.</li>
<li>The size (magnitude) of each component in that gradient indicates how important a change in that particular weight/bias is relative to the others.</li>
</ul>
</li>
<li>
<p><strong>Take a Small Step Downhill</strong></p>
<ul>
<li>Update each weight and bias by a small fraction of its negative gradient component.</li>
<li>These small steps ensure you approach a local minimum, and the step size often shrinks as you get closer (like a ball rolling into a valley, slowing down near the bottom).</li>
</ul>
</li>
<li>
<p><strong>Repeat</strong></p>
<ul>
<li>You continually re-compute the new gradient at each step (because once weights change, so do the network outputs and thus the cost landscape).</li>
<li>Keep iterating until changes no longer meaningfully reduce the cost, indicating convergence on a local minimum.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="backpropagation">Backpropagation<a href="#backpropagation" class="hash-link" aria-label="Direct link to Backpropagation" title="Direct link to Backpropagation">​</a></h3>
<p>The efficient algorithm used to calculate the gradient in a neural network is known as <strong>backpropagation</strong>. It systematically applies the chain rule of calculus to propagate errors backward - from the output layer to each hidden layer - assigning credit (or blame) to specific weights. This way, every single weight and bias can be nudged in just the right direction to reduce the overall cost.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4️⃣-performance-and-observations">4️⃣ <strong>Performance and Observations</strong><a href="#4️⃣-performance-and-observations" class="hash-link" aria-label="Direct link to 4️⃣-performance-and-observations" title="Direct link to 4️⃣-performance-and-observations">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accuracy-on-new-data">Accuracy on New Data<a href="#accuracy-on-new-data" class="hash-link" aria-label="Direct link to Accuracy on New Data" title="Direct link to Accuracy on New Data">​</a></h3>
<p>Once trained, the network is tested on new images it has never seen. A typical two-hidden-layer neural network (with 16 neurons each) can reach about 96% accuracy on the MNIST test set. With a few tweaks (e.g., more layers or different hyperparameters), it can climb to around 98%. While not the absolute state-of-the-art, this still demonstrates that the learned parameters generalize beyond the training data.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hidden-layer-interpretations">Hidden Layer Interpretations<a href="#hidden-layer-interpretations" class="hash-link" aria-label="Direct link to Hidden Layer Interpretations" title="Direct link to Hidden Layer Interpretations">​</a></h3>
<p>A simple hope was that hidden neurons might each specialize in detecting something like edges or loops. However, in practice, the learned weights for these layers can look random or obscure when visualized. The network still does well at classification; it simply finds its own sometimes-unintuitive way of partitioning the input space.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="overconfidence-on-garbage-inputs">Overconfidence on Garbage Inputs<a href="#overconfidence-on-garbage-inputs" class="hash-link" aria-label="Direct link to Overconfidence on Garbage Inputs" title="Direct link to Overconfidence on Garbage Inputs">​</a></h3>
<p>A trained network might respond confidently to random noise, predicting, for instance, a “5” when the input is purely random pixels. This happens because the network only ever sees neat, centered digit images during training. It has no reason (from the cost function’s perspective) to learn caution or produce “uncertain” outputs on unrecognizable inputs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5️⃣-local-minima-and-memorization-in-modern-dl">5️⃣ <strong>Local Minima and “Memorization” in Modern DL</strong><a href="#5️⃣-local-minima-and-memorization-in-modern-dl" class="hash-link" aria-label="Direct link to 5️⃣-local-minima-and-memorization-in-modern-dl" title="Direct link to 5️⃣-local-minima-and-memorization-in-modern-dl">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="local-minima-as-valleys">Local Minima as Valleys<a href="#local-minima-as-valleys" class="hash-link" aria-label="Direct link to Local Minima as Valleys" title="Direct link to Local Minima as Valleys">​</a></h3>
<p>When gradient descent is applied, it may settle into different local minima depending on how the network is initialized. Deep learning researchers have shown that in high-dimensional parameter spaces, many local minima might have similarly good performance on the training data.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="structured-vs-random-labels">Structured vs. Random Labels<a href="#structured-vs-random-labels" class="hash-link" aria-label="Direct link to Structured vs. Random Labels" title="Direct link to Structured vs. Random Labels">​</a></h3>
<p>A striking experiment involves randomizing the labels of a training set. Remarkably, large networks can memorize these random assignments, achieving near-perfect accuracy on the training data but failing entirely on real images (where the labels are correct). This memorization highlights that neural networks often have enough capacity to overfit without actually learning useful patterns.</p>
<ul>
<li>
<p><strong>Structured Data</strong><br>
<!-- -->When the labels do correspond to the right images, the cost function drops much more quickly, indicating the network is learning meaningful patterns rather than merely memorizing.</p>
</li>
<li>
<p><strong>Memorized Data</strong><br>
<!-- -->With randomly shuffled labels, training still converges (the network memorizes the random labels), but the cost decreases more slowly. No actual structure from the images is learned.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="implications">Implications<a href="#implications" class="hash-link" aria-label="Direct link to Implications" title="Direct link to Implications">​</a></h3>
<p>These experiments show that although neural networks can overfit to noise, they do tend to find more meaningful parameter configurations <em>faster</em> when the dataset is correctly labeled. Modern developments in deep learning build upon such insights, using regularization, data augmentation, or specialized architectures (like convolutional or recurrent networks) to detect more robust patterns and avoid memorizing noise.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6️⃣-beyond-the-basics">6️⃣ <strong>Beyond the Basics</strong><a href="#6️⃣-beyond-the-basics" class="hash-link" aria-label="Direct link to 6️⃣-beyond-the-basics" title="Direct link to 6️⃣-beyond-the-basics">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-continuous-activations">Why Continuous Activations?<a href="#why-continuous-activations" class="hash-link" aria-label="Direct link to Why Continuous Activations?" title="Direct link to Why Continuous Activations?">​</a></h3>
<p>To make gradient descent feasible, the network’s activations must be differentiable functions of their inputs. This is why sigmoid-like or ReLU activations are used instead of purely binary on/off switches (as in some simplified biological neuron models). If neuron outputs were discrete, the cost function would not be smooth, and there would be no easy way to compute a gradient.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-up">Scaling Up<a href="#scaling-up" class="hash-link" aria-label="Direct link to Scaling Up" title="Direct link to Scaling Up">​</a></h3>
<p>The approach to digit classification described here (two hidden layers, fully connected, 16 neurons each) is a small example by modern standards. Current state-of-the-art networks for image tasks often have many layers (dozens or even hundreds) and incorporate convolutional layers, pooling, skip connections, and more. Yet the same core ideas - gradient descent, backpropagation, cost functions - remain the bedrock of training.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7️⃣-conclusion">7️⃣ <strong>Conclusion</strong><a href="#7️⃣-conclusion" class="hash-link" aria-label="Direct link to 7️⃣-conclusion" title="Direct link to 7️⃣-conclusion">​</a></h2>
<p>In essence, the story of gradient descent in neural networks is a repeated calculus exercise:</p>
<ol>
<li>Define a cost function that measures how well (or poorly) the network is classifying the training data.</li>
<li>Compute the gradient - a measure of which parameter changes reduce that cost the fastest.</li>
<li>Nudge each parameter in the negative gradient’s direction, then repeat.</li>
</ol>
<p>This systematic “hill descent” drives the network’s weights and biases toward configurations that effectively map handwritten digits to their correct labels. While local minima and memorization raise subtle questions, especially in very large networks, the core principle of gradient descent remains foundational for deep learning. Through careful architectural design and regularization, modern networks learn powerful representations from vast datasets, performing tasks once deemed far beyond the reach of simple layered perceptrons.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">📖 Linear Regression</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/MindMash/docs/notes/Information Technology/Artificial Intelligence/👨🏻‍🏫 Google - 02 - ML/google-2-unit-2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">📖 Logistic Regression</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#️-useful-materials" class="table-of-contents__link toc-highlight">➡️ <strong>Useful Materials</strong></a><ul><li><a href="#original-source" class="table-of-contents__link toc-highlight">Original Source</a></li><li><a href="#resources-and-further-study" class="table-of-contents__link toc-highlight">Resources and Further Study</a></li></ul></li><li><a href="#1️⃣-background-and-motivation" class="table-of-contents__link toc-highlight">1️⃣ <strong>Background and Motivation</strong></a><ul><li><a href="#handwritten-digit-recognition" class="table-of-contents__link toc-highlight">Handwritten Digit Recognition</a></li><li><a href="#network-architecture-recap" class="table-of-contents__link toc-highlight">Network Architecture Recap</a></li><li><a href="#parameters-weights-and-biases" class="table-of-contents__link toc-highlight">Parameters (Weights and Biases)</a></li></ul></li><li><a href="#2️⃣-the-learning-goal-minimizing-cost" class="table-of-contents__link toc-highlight">2️⃣ <strong>The Learning Goal: Minimizing Cost</strong></a><ul><li><a href="#1-define-a-cost-function" class="table-of-contents__link toc-highlight">1. Define a Cost Function</a></li><li><a href="#2-average-over-the-entire-training-set" class="table-of-contents__link toc-highlight">2. Average Over the Entire Training Set</a></li><li><a href="#3-local-minima-vs-global-minima" class="table-of-contents__link toc-highlight">3. Local Minima vs. Global Minima</a></li></ul></li><li><a href="#3️⃣-the-core-algorithm-gradient-descent" class="table-of-contents__link toc-highlight">3️⃣ <strong>The Core Algorithm: Gradient Descent</strong></a><ul><li><a href="#key-idea" class="table-of-contents__link toc-highlight">Key Idea</a></li><li><a href="#backpropagation" class="table-of-contents__link toc-highlight">Backpropagation</a></li></ul></li><li><a href="#4️⃣-performance-and-observations" class="table-of-contents__link toc-highlight">4️⃣ <strong>Performance and Observations</strong></a><ul><li><a href="#accuracy-on-new-data" class="table-of-contents__link toc-highlight">Accuracy on New Data</a></li><li><a href="#hidden-layer-interpretations" class="table-of-contents__link toc-highlight">Hidden Layer Interpretations</a></li><li><a href="#overconfidence-on-garbage-inputs" class="table-of-contents__link toc-highlight">Overconfidence on Garbage Inputs</a></li></ul></li><li><a href="#5️⃣-local-minima-and-memorization-in-modern-dl" class="table-of-contents__link toc-highlight">5️⃣ <strong>Local Minima and “Memorization” in Modern DL</strong></a><ul><li><a href="#local-minima-as-valleys" class="table-of-contents__link toc-highlight">Local Minima as Valleys</a></li><li><a href="#structured-vs-random-labels" class="table-of-contents__link toc-highlight">Structured vs. Random Labels</a></li><li><a href="#implications" class="table-of-contents__link toc-highlight">Implications</a></li></ul></li><li><a href="#6️⃣-beyond-the-basics" class="table-of-contents__link toc-highlight">6️⃣ <strong>Beyond the Basics</strong></a><ul><li><a href="#why-continuous-activations" class="table-of-contents__link toc-highlight">Why Continuous Activations?</a></li><li><a href="#scaling-up" class="table-of-contents__link toc-highlight">Scaling Up</a></li></ul></li><li><a href="#7️⃣-conclusion" class="table-of-contents__link toc-highlight">7️⃣ <strong>Conclusion</strong></a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">My Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://danielepassabi.github.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My Portfolio<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/daniele-passabi/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">(I do not really need a) Copyright © 2025 MindMash, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>